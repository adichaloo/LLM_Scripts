{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c36a2-4f38-40d4-9256-a851b1643ff1",
   "metadata": {
    "id": "7c8c36a2-4f38-40d4-9256-a851b1643ff1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648608b-863d-4ae7-bc9c-4d87eb2c233a",
   "metadata": {
    "id": "c648608b-863d-4ae7-bc9c-4d87eb2c233a"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2331e-32a5-4638-8c9d-36b73fefebdb",
   "metadata": {
    "id": "4af2331e-32a5-4638-8c9d-36b73fefebdb"
   },
   "outputs": [],
   "source": [
    "class GatingMechanism(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingMechanism, self).__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(dim=1)\n",
    "        gate_scores = F.softmax(self.gate(x_mean), dim=-1)  # Shape: [batch_size, num_experts]\n",
    "        return gate_scores.argmax(dim=-1)  # Shape: [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9cfc6-f245-4963-82cc-7c45d82259f3",
   "metadata": {
    "id": "cfb9cfc6-f245-4963-82cc-7c45d82259f3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MoEModelWithPooling(nn.Module):\n",
    "    def __init__(self, experts, input_dim):\n",
    "        super().__init__()\n",
    "        self.experts = experts\n",
    "        # Update to access layers based on the specific model description shared\n",
    "        self.num_layers = len(experts[0].base_model.model.model.layers)  # Correct path to access layers\n",
    "        self.gating = GatingMechanism(input_dim, len(experts))\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1).to(device)  # Example pooling layer\n",
    "        self.output_layer = nn.Linear(4096, 4).to(device)\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Update to correctly access the initial embedding layer\n",
    "        # Assuming wte is the embedding layer, adjust if different in your model\n",
    "        # print(input_ids)\n",
    "        x = self.experts[0].base_model.model.model.embed_tokens(input_ids)\n",
    "        # print('x', x.shape)\n",
    "        for i in range(self.num_layers):\n",
    "            expert_indices = self.gating(x)\n",
    "            # print('x', x.shape)\n",
    "            layer_output = torch.zeros_like(x)\n",
    "            # print('x', x.shape)\n",
    "            for idx, expert in enumerate(self.experts):\n",
    "                mask = (expert_indices == idx).unsqueeze(-1).unsqueeze(1).half()\n",
    "                # print('mask', mask.shape)\n",
    "                expert_input = x * mask\n",
    "                # print('expert_input', expert_input.shape, expert_input.dtype)\n",
    "                # Accessing the i-th layer correctly according to the model structure\n",
    "                # exp_out = expert.base_model.model.model.layers[i]\n",
    "                # print(exp_out)\n",
    "                expert_output = expert.base_model.model.model.layers[i](expert_input)[0]\n",
    "                # print('expert_output', expert_output.shape, expert_output.dtype)\n",
    "                layer_output += expert_output * mask\n",
    "            x = layer_output\n",
    "\n",
    "        # print('x1',x.shape)\n",
    "        x = x.transpose(1, 2)  # Adjust dimensions for pooling\n",
    "        x = self.pooling(x).squeeze(2)\n",
    "        # print('x2', x.shape)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# GatingMechanism definition assumed to be implemented elsewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6F9nhynt5wMV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6F9nhynt5wMV",
    "outputId": "350d9f3c-d95b-4278-d528-1ee6ba4d1da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-crdv267y/unsloth_c2aa386fdb3b40af955a8cfd4ae05667\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-crdv267y/unsloth_c2aa386fdb3b40af955a8cfd4ae05667\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 47ffd39abd02338e8a5f226d0f529347fb7e5f89\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.4)\n",
      "Requirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.40.2)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Collecting xformers<0.0.26\n",
      "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl\n",
      "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n",
      "Successfully installed accelerate-0.30.1 bitsandbytes-0.43.1 peft-0.10.0 trl-0.8.6 xformers-0.0.25.post1\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00714f50-65fa-495f-b8de-04ee52f057ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00714f50-65fa-495f-b8de-04ee52f057ea",
    "outputId": "b359969d-ae5d-4d0a-febb-f7d500ff37e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla V100-PCIE-16GB. Max memory: 15.773 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla V100-PCIE-16GB. Max memory: 15.773 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained models\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model1, tokenizer = FastLanguageModel.from_pretrained(\"unsloth_domain\",\n",
    "                                                     max_seq_length=max_seq_length,\n",
    "                                                     dtype=dtype,\n",
    "                                                     load_in_4bit=load_in_4bit)\n",
    "\n",
    "model2, tokenizer = FastLanguageModel.from_pretrained(\"ai2_arc_instruction_tuned_mistral_7b\",\n",
    "                                                     max_seq_length=max_seq_length,\n",
    "                                                     dtype=dtype,\n",
    "                                                     load_in_4bit=load_in_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f26286-ead9-4963-a9a3-bf585643beb9",
   "metadata": {
    "id": "32f26286-ead9-4963-a9a3-bf585643beb9"
   },
   "outputs": [],
   "source": [
    "for param in model1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "models = [model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc4a0a-32b6-4726-9deb-ea44858d4216",
   "metadata": {
    "id": "46dc4a0a-32b6-4726-9deb-ea44858d4216"
   },
   "outputs": [],
   "source": [
    "num_layers = len(model1.base_model.model.model.layers)\n",
    "\n",
    "moe_model = MoEModelWithPooling(models, input_dim=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae064d-5ab4-46a6-91cf-8ca44cb421e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ae064d-5ab4-46a6-91cf-8ca44cb421e7",
    "outputId": "2b520e41-76ea-4cc9-fd1a-df8ea03d9552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoEModelWithPooling(\n",
      "  (gating): GatingMechanism(\n",
      "    (gate): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      "  (pooling): AdaptiveAvgPool1d(output_size=1)\n",
      "  (output_layer): Linear(in_features=4096, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(moe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf72f79-3fba-4310-85fe-99f777efd4b0",
   "metadata": {
    "id": "8bf72f79-3fba-4310-85fe-99f777efd4b0"
   },
   "outputs": [],
   "source": [
    "# from torch.cuda.amp import GradScaler\n",
    "\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# temp = torch.ones((2,8), dtype=torch.int64).to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# labels = torch.rand(2, 4).float().to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(moe_model.parameters(), lr=1e-3)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# with torch.cuda.amp.autocast():\n",
    "#     output = moe_model(temp).float()\n",
    "#     loss = criterion(output, labels.float())\n",
    "\n",
    "# scaler.scale(loss).backward()\n",
    "# scaler.step(optimizer)\n",
    "# scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HdvpSKpvy2d_",
   "metadata": {
    "id": "HdvpSKpvy2d_"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pe8PHYbH2ncc",
   "metadata": {
    "id": "Pe8PHYbH2ncc"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset\n",
    "\n",
    "dataset_location = 'medmcqa-prompts'\n",
    "\n",
    "train_dataset = load_from_disk(f\"{dataset_location}/train_prompts.hf\")\n",
    "# test_dataset = load_from_disk(f\"{dataset_location}/test_prompts.hf\")\n",
    "eval_dataset = load_from_disk(f\"{dataset_location}/eval_prompts.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Km-2uA852uY2",
   "metadata": {
    "id": "Km-2uA852uY2"
   },
   "outputs": [],
   "source": [
    "# train = []\n",
    "# val = []\n",
    "# count = 0\n",
    "# for i in train_dataset:\n",
    "#     train.append(i)\n",
    "#     count += 1\n",
    "#     if count >= 100:\n",
    "#         break\n",
    "\n",
    "# count = 0\n",
    "# for i in eval_dataset:\n",
    "#     val.append(i)\n",
    "#     count += 1\n",
    "#     if count >= 100:\n",
    "#         break\n",
    "\n",
    "# train_dataset = ''\n",
    "# eval_dataset = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HQuiO-Kb2wlO",
   "metadata": {
    "id": "HQuiO-Kb2wlO"
   },
   "outputs": [],
   "source": [
    "# print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MIfB2l0j2x7c",
   "metadata": {
    "id": "MIfB2l0j2x7c"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MCQDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  # Changed to float for one-hot encoding\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Function to encode the data\n",
    "def encode_data(tokenizer, prompts):\n",
    "    encodings = tokenizer(prompts, truncation=True, padding=True, max_length=128)\n",
    "    return encodings\n",
    "\n",
    "# Prepare the data for tokenization\n",
    "prompts = [item['prompt'] for item in train_dataset]\n",
    "labels = [item['label_one_hot'] for item in train_dataset]  # one-hot encoded labels\n",
    "\n",
    "# Tokenize data\n",
    "encodings = encode_data(tokenizer, prompts)\n",
    "\n",
    "# Create dataset\n",
    "train_set = MCQDataset(encodings, labels)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=6, shuffle=True)\n",
    "\n",
    "\n",
    "prompts = [item['prompt'] for item in eval_dataset]\n",
    "labels = [item['label_one_hot'] for item in eval_dataset]  # one-hot encoded labels\n",
    "\n",
    "# Tokenize data\n",
    "encodings = encode_data(tokenizer, prompts)\n",
    "\n",
    "# Create dataset\n",
    "eval_set = MCQDataset(encodings, labels)\n",
    "\n",
    "# DataLoader\n",
    "val_loader = DataLoader(eval_set, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zj9aBgOW2yL_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zj9aBgOW2yL_",
    "outputId": "7ceb5a87-ceb9-4f15-ae6c-c4c75aff68d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Input IDs: torch.Size([6, 128])\n",
      "Attention Mask: torch.Size([6, 128])\n",
      "Labels: torch.Size([6, 4])\n",
      "First input IDs example: tensor([    1, 28705,    13,  2287, 22478, 28747,    13,  2287,   330,  7749,\n",
      "         7567,   395,  8039,  1212,   890,  2654,   265, 28708,   395,  4242,\n",
      "        28723,  1418,  4819, 28719,   806,   385,  6541, 28725,   277,  2857,\n",
      "          335, 13441,  3042,  1419,   304,  2823,  2458, 28723,  1418, 19869,\n",
      "         5643,   302,  3042, 28725,   690,  1235,   459,   506,   430,  4206,\n",
      "          504,   294,   277,   672, 28804,    13,  4018, 28747,    13, 28741,\n",
      "        28723,  7787,   509,    13, 28760, 28723,   334,   586, 28717,    13,\n",
      "        28743, 28723,  8990,  2737,   270,   455,   570,  1140,   375,   425,\n",
      "          472,    13, 28757, 28723,  1234, 28726, 28760, 28750,    13,    13,\n",
      "         2287,   733, 16289, 28793,   318,  5303,   456,  9713,  4191, 27710,\n",
      "        22478,   304,  3084,   272,  4714,  3551,   575,   302,  2308,  2877,\n",
      "        28732, 28741, 28725, 28760, 28725, 28743, 28725, 28757, 28731,   390,\n",
      "          272,  4372, 28723,   733, 28748, 16289, 28793,    13])\n",
      "First attention mask example: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "First label example: tensor([1., 0., 0., 0.])\n",
      "Batch 1\n",
      "Input IDs: torch.Size([6, 128])\n",
      "Attention Mask: torch.Size([6, 128])\n",
      "Labels: torch.Size([6, 4])\n",
      "Batch 2\n",
      "Input IDs: torch.Size([6, 128])\n",
      "Attention Mask: torch.Size([6, 128])\n",
      "Labels: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(\"Batch\", i)\n",
    "    print(\"Input IDs:\", batch['input_ids'].shape)\n",
    "    print(\"Attention Mask:\", batch['attention_mask'].shape)\n",
    "    print(\"Labels:\", batch['labels'].shape)\n",
    "\n",
    "    # Print the actual content of the first example in the batch\n",
    "    if i == 0:\n",
    "        print(\"First input IDs example:\", batch['input_ids'][0])\n",
    "        print(\"First attention mask example:\", batch['attention_mask'][0])\n",
    "        print(\"First label example:\", batch['labels'][0])\n",
    "\n",
    "    # Optionally, break after a few batches to avoid too much output\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wz2uVnHJ2zop",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "Wz2uVnHJ2zop",
    "outputId": "336d2cdb-cb82-415a-b1d5-a4f13b138b33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:   0%|          | 0/30471 [00:03<?, ?batch/s, loss=1.39, temp_acc=33.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3887276649475098\n",
      "Temp accuracy:  33.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:   0%|          | 0/30471 [01:55<?, ?batch/s, loss=1.34, temp_acc=27.4]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "def compute_loss(outputs, labels, attention_mask):\n",
    "    # Flatten the outputs and labels for loss calculation\n",
    "    active_loss = attention_mask.view(-1) == 1  # Mask out padded tokens\n",
    "    active_logits = outputs.view(-1, outputs.size(-1))[active_loss]\n",
    "    active_labels = labels.view(-1)[active_loss]\n",
    "    return F.cross_entropy(active_logits, active_labels)\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, epochs=3):\n",
    "    scaler = GradScaler()\n",
    "    device = torch.device(\"cuda\")\n",
    "    # model = model.to(device)  # Ensures model and all submodules are float32\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        train_samples = 0\n",
    "\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [TRAIN]\", unit=\"batch\")\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            train_samples += labels.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(input_ids).float()\n",
    "                loss = criterion(output, labels.float())\n",
    "                predictions = torch.argmax(output, dim=1)\n",
    "                labels_indices = torch.argmax(labels, dim=1)\n",
    "                # print(output)\n",
    "                # print(predictions)\n",
    "                # print(labels)\n",
    "                total_train_correct += (predictions == labels_indices).sum().item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            train_pbar.set_postfix(loss=loss.item(), temp_acc=100 * total_train_correct / train_samples)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(i, loss.item())\n",
    "                print(f\"Temp accuracy: \", total_train_correct / train_samples * 100)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = total_train_correct / train_samples * 100\n",
    "        print(f\"Training Accuracy: \", train_accuracy)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_train_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss, val_samples, total_val_correct = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader):\n",
    "                input_ids, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_ids).float()\n",
    "                    val_loss = criterion(outputs, labels.float())\n",
    "                    predictions = torch.argmax(outputs, dim=1)\n",
    "                    total_val_correct += (predictions == labels).sum().item()\n",
    "\n",
    "                total_val_loss += val_loss.item()\n",
    "                val_samples += labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = total_val_correct / val_samples * 100\n",
    "        print(f\"Validation Accuracy: \", val_accuracy)\n",
    "        print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "train_and_validate(moe_model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a191f74-46cd-4267-aa6c-2ce1760b5180",
   "metadata": {
    "id": "1a191f74-46cd-4267-aa6c-2ce1760b5180"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
